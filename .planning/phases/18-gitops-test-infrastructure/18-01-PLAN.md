---
phase: 18-gitops-test-infrastructure
plan: 18-01
title: Minimal Test VMs for GitOps Validation
depends_on:
  - Phase 15 (Self-Managing Infrastructure)
  - Phase 16 (SOPS Key Management)
status: not_started
---

# Plan 18-01: Minimal Test VMs for GitOps Validation

## Objective

Create two ultra-minimal headless VMs (**sorrow** and **torment**) specifically for testing the GitOps infrastructure before relying on it in production. These VMs will validate multi-host jj operations, auto-upgrade workflows, and decentralized configuration management.

**Why This Matters**: Phase 15 introduced complex GitOps features (chezmoi+jj sync, auto-upgrade with validation, golden generation rollback). Before deploying to production hosts, we need isolated test environments to validate:
- Multi-host jj concurrent edits (conflict-free merge)
- Auto-upgrade workflow end-to-end
- Build validation and rollback
- Golden generation recovery
- Network failure graceful degradation

## Success Criteria

### Host Creation
- [ ] Host configs created: `hosts/sorrow/default.nix` and `hosts/torment/default.nix`
- [ ] Hardware configs generated: `hosts/sorrow/hardware-configuration.nix` and `hosts/torment/hardware-configuration.nix`
- [ ] Both hosts build successfully
- [ ] Both hosts added to flake outputs

### Deployment
- [ ] VMs deployed and accessible via SSH
- [ ] VMs accessible via Tailscale
- [ ] Core services running (sshd, tailscaled, atuin, syncthing)
- [ ] No graphical environment (headless)
- [ ] Fast rebuild time (< 2 minutes)

### GitOps Infrastructure
- [ ] Auto-upgrade configured (mode: local, with validation)
- [ ] Golden generation rollback enabled
- [ ] SOPS secrets working (minimal secrets only)
- [ ] Ready for Phase 15-03c testing (chezmoi+jj sync)

## Context

**Griefling Configuration** (current test VM):
- Role: `["vm", "test"]`
- Includes: hyprland, ly, full desktop stack
- Problem: Too heavy for rapid testing
- Build time: ~5 minutes

**Desired Configuration** (sorrow/torment):
- Role: `["vm"]` only (skip desktop)
- Headless: No display manager, no desktop environment
- Core services only: SSH, Tailscale, atuin, syncthing
- Build time goal: < 2 minutes
- Deployment: Quick iteration for testing

**GitOps Features to Test**:
1. Multi-host jj concurrent edits (edit on sorrow, edit on torment simultaneously)
2. Auto-upgrade with build validation (inject syntax error, verify rollback)
3. Network failure handling (disconnect, verify graceful degradation)
4. Golden generation rollback (break boot, verify auto-recovery)
5. Full workflow: config change → pull → build → validate → switch

## Files to Reference

- `hosts/griefling/default.nix` - Current test VM (too heavy)
- `roles/form-vm.nix` - VM role with desktop modules
- `modules/common/auto-upgrade.nix` - Auto-upgrade configuration
- `modules/system/boot/golden-generation.nix` - Rollback system
- `modules/services/dotfiles/chezmoi-sync.nix` - Chezmoi sync (Phase 15-03a)

## Implementation Tasks

### Task 1: Create Minimal VM Role Extension

**Type**: Configuration
**Files**: Create `roles/form-vm-headless.nix` (optional) OR use module overrides in host

**Action**:
We have two options:

**Option A: Use Module Overrides in Host Configs** (Recommended - simpler):
```nix
# hosts/sorrow/default.nix
roles = [ "vm" ]; # Gets base VM config

# Override to remove desktop modules
modules.services.desktop = lib.mkForce [];
modules.services.display-manager = lib.mkForce [];

# Keep only headless services
modules.services.cli = [ "atuin" ];
modules.services.networking = [
  "openssh"
  "ssh"
  "syncthing"
  "tailscale"
];
```

**Option B: Create Headless VM Role** (cleaner separation):
```nix
# roles/form-vm-headless.nix
# Headless VM role - no desktop, display manager, or graphical stack
# Perfect for testing infrastructure without GUI overhead
{ config, lib, ... }:
{
  config = lib.mkIf (builtins.elem "vm-headless" config.roles) {
    modules = {
      apps.cli = [ "comma" "shell" "tools" ];
      services = {
        cli = [ "atuin" ];
        networking = [
          "openssh"
          "ssh"
          "syncthing"
          "tailscale"
        ];
      };
    };

    # VM hardware (same as form-vm.nix)
    boot.loader = {
      systemd-boot.enable = lib.mkDefault true;
      efi.canTouchEfiVariables = lib.mkDefault true;
      timeout = lib.mkDefault 3;
    };

    boot.initrd = {
      systemd.enable = lib.mkDefault true;
      kernelModules = lib.mkDefault [
        "xhci_pci" "ohci_pci" "ehci_pci" "virtio_pci"
        "ahci" "usbhid" "sr_mod" "virtio_blk"
      ];
    };

    boot.kernelParams = lib.mkDefault [ "console=tty1" "console=ttyS0,115200" ];
    boot.kernelModules = lib.mkDefault [ "virtio-gpu" "bochs_drm" ];

    # Headless - no video drivers needed
    # VM utilities
    services.qemuGuest.enable = lib.mkDefault true;
    services.spice-vdagentd.enable = lib.mkDefault true;

    # Networking
    networking.useDHCP = lib.mkDefault true;

    # Minimal secret categories
    hostSpec.secretCategories = lib.mkDefault [ "base" ];
  };
}
```

**Decision**: Use Option A (module overrides) for simplicity. No new role file needed.

**Verify**:
```bash
# Check that vm role exists
cat roles/form-vm.nix | grep "config = lib.mkIf"

# Verify module structure
ls modules/services/cli/
ls modules/services/networking/
```

**Done when**:
- [ ] Decision made on which option to use
- [ ] If Option B: `roles/form-vm-headless.nix` created

### Task 2: Create Host Configuration for Sorrow

**Type**: Host Creation
**Files**:
- `hosts/sorrow/default.nix` (create)
- `hosts/sorrow/hardware-configuration.nix` (create)

**Action**:

1. **Create directory**:
   ```bash
   mkdir -p hosts/sorrow
   ```

2. **Create host configuration** (`hosts/sorrow/default.nix`):
   ```nix
   # Sorrow - Minimal headless VM for GitOps testing
   # Purpose: Test multi-host jj sync, auto-upgrade, and infrastructure features
   { lib, ... }:
   {
     imports = [ ./hardware-configuration.nix ];

     # Disk configuration via modules/disks
     disks = {
       enable = true;
       layout = "btrfs";
       device = "/dev/vda";
       withSwap = false;
     };

     # ========================================
     # ROLE SELECTION
     # ========================================
     roles = [ "vm" ]; # Base VM role

     # ========================================
     # MODULE OVERRIDES (remove desktop)
     # ========================================
     # Remove desktop modules for headless operation
     modules.services.desktop = lib.mkForce [];
     modules.services.display-manager = lib.mkForce [];

     # Keep only essential headless services
     modules.services.cli = [ "atuin" ];
     modules.services.networking = [
       "openssh"
       "ssh"
       "syncthing"
       "tailscale"
     ];

     # ========================================
     # HOST IDENTITY
     # ========================================
     hostSpec = {
       hostName = "sorrow";
       primaryUsername = "rain";
     };

     # ========================================
     # AUTO-UPGRADE (GitOps testing)
     # ========================================
     myModules.services.autoUpgrade = {
       enable = true;
       mode = "local";
       schedule = "hourly"; # Frequent for testing

       # Safety features (Phase 15-03b)
       buildBeforeSwitch = true;
       validationChecks = [
         "systemctl --quiet is-enabled sshd"
         "systemctl --quiet is-enabled tailscaled"
       ];
       onValidationFailure = "rollback";
     };

     # ========================================
     # GOLDEN GENERATION (boot safety)
     # ========================================
     myModules.system.boot.goldenGeneration = {
       enable = true;
       validateServices = [
         "sshd.service"
         "tailscaled.service"
       ];
       autoPinAfterBoot = true;
     };
   }
   ```

3. **Create minimal hardware configuration** (`hosts/sorrow/hardware-configuration.nix`):
   ```nix
   # Hardware configuration for sorrow VM
   # Generated for QEMU/KVM virtual machine
   { config, lib, pkgs, modulesPath, ... }:
   {
     imports = [ (modulesPath + "/profiles/qemu-guest.nix") ];

     boot.initrd.availableKernelModules = [
       "ata_piix" "uhci_hcd" "virtio_pci" "virtio_scsi" "sd_mod" "sr_mod"
     ];
     boot.initrd.kernelModules = [ ];
     boot.kernelModules = [ ];
     boot.extraModulePackages = [ ];

     # Filesystems managed by disko (see default.nix)
     fileSystems."/" = lib.mkDefault {
       device = "/dev/disk/by-label/nixos";
       fsType = "btrfs";
       options = [ "subvol=@root" "compress=zstd" "noatime" ];
     };

     fileSystems."/nix" = lib.mkDefault {
       device = "/dev/disk/by-label/nixos";
       fsType = "btrfs";
       options = [ "subvol=@nix" "compress=zstd" "noatime" ];
     };

     fileSystems."/boot" = lib.mkDefault {
       device = "/dev/disk/by-label/boot";
       fsType = "vfat";
     };

     nixpkgs.hostPlatform = lib.mkDefault "x86_64-linux";
   }
   ```

**Verify**:
```bash
# Check host config
ls hosts/sorrow/
cat hosts/sorrow/default.nix | grep -E "hostName|roles"

# Verify module structure is correct
nix eval .#nixosConfigurations.sorrow.config.networking.hostName
```

**Done when**:
- [ ] `hosts/sorrow/` directory created
- [ ] `hosts/sorrow/default.nix` created with headless config
- [ ] `hosts/sorrow/hardware-configuration.nix` created

### Task 3: Create Host Configuration for Torment

**Type**: Host Creation
**Files**:
- `hosts/torment/default.nix` (create)
- `hosts/torment/hardware-configuration.nix` (create)

**Action**:

1. **Create directory**:
   ```bash
   mkdir -p hosts/torment
   ```

2. **Create host configuration** (`hosts/torment/default.nix`):
   ```nix
   # Torment - Minimal headless VM for GitOps testing
   # Purpose: Test multi-host jj sync, auto-upgrade, and infrastructure features
   { lib, ... }:
   {
     imports = [ ./hardware-configuration.nix ];

     # Disk configuration via modules/disks
     disks = {
       enable = true;
       layout = "btrfs";
       device = "/dev/vda";
       withSwap = false;
     };

     # ========================================
     # ROLE SELECTION
     # ========================================
     roles = [ "vm" ]; # Base VM role

     # ========================================
     # MODULE OVERRIDES (remove desktop)
     # ========================================
     # Remove desktop modules for headless operation
     modules.services.desktop = lib.mkForce [];
     modules.services.display-manager = lib.mkForce [];

     # Keep only essential headless services
     modules.services.cli = [ "atuin" ];
     modules.services.networking = [
       "openssh"
       "ssh"
       "syncthing"
       "tailscale"
     ];

     # ========================================
     # HOST IDENTITY
     # ========================================
     hostSpec = {
       hostName = "torment";
       primaryUsername = "rain";
     };

     # ========================================
     # AUTO-UPGRADE (GitOps testing)
     # ========================================
     myModules.services.autoUpgrade = {
       enable = true;
       mode = "local";
       schedule = "hourly"; # Frequent for testing

       # Safety features (Phase 15-03b)
       buildBeforeSwitch = true;
       validationChecks = [
         "systemctl --quiet is-enabled sshd"
         "systemctl --quiet is-enabled tailscaled"
       ];
       onValidationFailure = "rollback";
     };

     # ========================================
     # GOLDEN GENERATION (boot safety)
     # ========================================
     myModules.system.boot.goldenGeneration = {
       enable = true;
       validateServices = [
         "sshd.service"
         "tailscaled.service"
       ];
       autoPinAfterBoot = true;
     };
   }
   ```

3. **Create minimal hardware configuration** (`hosts/torment/hardware-configuration.nix`):
   ```nix
   # Hardware configuration for torment VM
   # Generated for QEMU/KVM virtual machine
   { config, lib, pkgs, modulesPath, ... }:
   {
     imports = [ (modulesPath + "/profiles/qemu-guest.nix") ];

     boot.initrd.availableKernelModules = [
       "ata_piix" "uhci_hcd" "virtio_pci" "virtio_scsi" "sd_mod" "sr_mod"
     ];
     boot.initrd.kernelModules = [ ];
     boot.kernelModules = [ ];
     boot.extraModulePackages = [ ];

     # Filesystems managed by disko (see default.nix)
     fileSystems."/" = lib.mkDefault {
       device = "/dev/disk/by-label/nixos";
       fsType = "btrfs";
       options = [ "subvol=@root" "compress=zstd" "noatime" ];
     };

     fileSystems."/nix" = lib.mkDefault {
       device = "/dev/disk/by-label/nixos";
       fsType = "btrfs";
       options = [ "subvol=@nix" "compress=zstd" "noatime" ];
     };

     fileSystems."/boot" = lib.mkDefault {
       device = "/dev/disk/by-label/boot";
       fsType = "vfat";
     };

     nixpkgs.hostPlatform = lib.mkDefault "x86_64-linux";
   }
   ```

**Verify**:
```bash
# Check host config
ls hosts/torment/
cat hosts/torment/default.nix | grep -E "hostName|roles"

# Verify module structure is correct
nix eval .#nixosConfigurations.torment.config.networking.hostName
```

**Done when**:
- [ ] `hosts/torment/` directory created
- [ ] `hosts/torment/default.nix` created with headless config
- [ ] `hosts/torment/hardware-configuration.nix` created

### Task 4: Add Hosts to Flake Outputs

**Type**: Integration
**Files**: `flake.nix` (modify)

**Action**:

1. **Read current flake.nix** to find where hosts are defined:
   ```bash
   grep -n "nixosConfigurations" flake.nix | head -5
   ```

2. **Add sorrow and torment to nixosConfigurations**:

   Find the section that looks like:
   ```nix
   nixosConfigurations = {
     malphas = mkHost { hostname = "malphas"; };
     guppy = mkHost { hostname = "guppy"; };
     griefling = mkHost { hostname = "griefling"; };
     # ... other hosts
   };
   ```

   Add:
   ```nix
   nixosConfigurations = {
     malphas = mkHost { hostname = "malphas"; };
     guppy = mkHost { hostname = "guppy"; };
     griefling = mkHost { hostname = "griefling"; };
     sorrow = mkHost { hostname = "sorrow"; };     # NEW
     torment = mkHost { hostname = "torment"; };   # NEW
     # ... other hosts
   };
   ```

3. **Verify flake syntax**:
   ```bash
   nix flake check --no-build
   ```

**Verify**:
```bash
# List all nixosConfigurations
nix flake show | grep -A20 "nixosConfigurations"

# Verify sorrow and torment appear
nix flake show | grep -E "sorrow|torment"
```

**Done when**:
- [ ] `flake.nix` modified to include sorrow and torment
- [ ] `nix flake check` passes
- [ ] Both hosts visible in `nix flake show`

### Task 5: Build and Verify Configurations

**Type**: Validation
**Files**: None (build test)

**Action**:

1. **Build sorrow configuration**:
   ```bash
   echo "Building sorrow..."
   nix build .#nixosConfigurations.sorrow.config.system.build.toplevel --no-link
   ```

2. **Build torment configuration**:
   ```bash
   echo "Building torment..."
   nix build .#nixosConfigurations.torment.config.system.build.toplevel --no-link
   ```

3. **Verify both build successfully** (no errors)

4. **Check build time** (should be < 2 minutes each):
   ```bash
   time nix build .#nixosConfigurations.sorrow.config.system.build.toplevel --no-link --rebuild
   ```

5. **Verify module selections**:
   ```bash
   # Should show NO desktop modules
   nix eval .#nixosConfigurations.sorrow.config.modules.services.desktop --json
   # Expected: []

   # Should show headless services
   nix eval .#nixosConfigurations.sorrow.config.modules.services.cli --json
   # Expected: ["atuin"]

   nix eval .#nixosConfigurations.sorrow.config.modules.services.networking --json
   # Expected: ["openssh" "ssh" "syncthing" "tailscale"]
   ```

6. **Verify auto-upgrade enabled**:
   ```bash
   nix eval .#nixosConfigurations.sorrow.config.myModules.services.autoUpgrade.enable
   # Expected: true
   ```

7. **Verify golden generation enabled**:
   ```bash
   nix eval .#nixosConfigurations.sorrow.config.myModules.system.boot.goldenGeneration.enable
   # Expected: true
   ```

**Verify**:
```bash
# Quick verification script
for host in sorrow torment; do
  echo "=== Testing $host ==="
  nix build .#nixosConfigurations.$host.config.system.build.toplevel --no-link
  echo "✓ Build succeeded"

  desktop=$(nix eval .#nixosConfigurations.$host.config.modules.services.desktop --json)
  if [ "$desktop" = "[]" ]; then
    echo "✓ No desktop modules (headless)"
  else
    echo "✗ Desktop modules found: $desktop"
  fi

  autoupgrade=$(nix eval .#nixosConfigurations.$host.config.myModules.services.autoUpgrade.enable)
  if [ "$autoupgrade" = "true" ]; then
    echo "✓ Auto-upgrade enabled"
  else
    echo "✗ Auto-upgrade not enabled"
  fi
done
```

**Done when**:
- [ ] Sorrow builds successfully
- [ ] Torment builds successfully
- [ ] Build time < 2 minutes per host
- [ ] No desktop modules enabled (headless confirmed)
- [ ] Auto-upgrade and golden generation enabled
- [ ] All verification checks pass

### Task 6: Create VM Deployment Scripts

**Type**: Automation
**Files**: `scripts/deploy-test-vm.sh` (create)

**Action**:

Create helper script for deploying test VMs quickly:

```bash
#!/usr/bin/env bash
# scripts/deploy-test-vm.sh
# Quick deployment script for test VMs (sorrow, torment)

set -euo pipefail

VM_NAME="${1:-sorrow}"
DISK_SIZE="${2:-20G}"
MEMORY="${3:-2G}"
CPUS="${4:-2}"

case "$VM_NAME" in
  sorrow|torment)
    ;;
  *)
    echo "Usage: $0 <sorrow|torment> [disk-size] [memory] [cpus]"
    echo "Example: $0 sorrow 20G 2G 2"
    exit 1
    ;;
esac

echo "=== Deploying $VM_NAME test VM ==="
echo "Disk: $DISK_SIZE, Memory: $MEMORY, CPUs: $CPUS"

# Create VM disk
IMG_DIR="$HOME/vms"
mkdir -p "$IMG_DIR"
IMG_PATH="$IMG_DIR/$VM_NAME.qcow2"

if [ ! -f "$IMG_PATH" ]; then
  echo "Creating disk image: $IMG_PATH"
  qemu-img create -f qcow2 "$IMG_PATH" "$DISK_SIZE"
else
  echo "Using existing disk: $IMG_PATH"
fi

# Build installation ISO
echo "Building NixOS installer..."
nix build .#nixosConfigurations.$VM_NAME.config.system.build.isoImage --out-link result-$VM_NAME-iso

# Start VM for installation
echo "Starting VM for installation..."
echo "Connect via VNC on localhost:5900 or use QEMU monitor"
qemu-system-x86_64 \
  -enable-kvm \
  -m "$MEMORY" \
  -smp "$CPUS" \
  -drive file="$IMG_PATH",format=qcow2 \
  -cdrom result-$VM_NAME-iso/iso/*.iso \
  -boot d \
  -vnc :0 \
  -monitor stdio

echo "Installation complete. To start the VM:"
echo "  qemu-system-x86_64 -enable-kvm -m $MEMORY -smp $CPUS -drive file=$IMG_PATH,format=qcow2 -net nic -net user,hostfwd=tcp::2222-:22 -daemonize"
```

**Alternative: Use existing justfile commands**:

If `justfile` already has VM deployment commands, use those instead. Check:
```bash
grep -A5 "vm-deploy\|vm-create" justfile
```

**Verify**:
```bash
# Make script executable
chmod +x scripts/deploy-test-vm.sh

# Test help message
./scripts/deploy-test-vm.sh
```

**Done when**:
- [ ] Deployment script created (or existing justfile commands identified)
- [ ] Script is executable
- [ ] Help message works

### Task 7: Update Justfile with Test VM Commands

**Type**: Integration
**Files**: `justfile` (modify)

**Action**:

Add convenient commands for managing test VMs:

```make
# Build test VM configurations
build-test-vms:
  @echo "Building sorrow..."
  nix build .#nixosConfigurations.sorrow.config.system.build.toplevel --no-link
  @echo "Building torment..."
  nix build .#nixosConfigurations.torment.config.system.build.toplevel --no-link
  @echo "✓ Test VMs built successfully"

# Verify test VM configurations
verify-test-vms:
  @echo "Verifying sorrow and torment configurations..."
  @for host in sorrow torment; do \
    echo "=== $host ==="; \
    nix eval .#nixosConfigurations.$host.config.networking.hostName --raw; \
    echo " ✓"; \
  done

# Deploy test VM
deploy-test-vm vm:
  @echo "Deploying {{vm}} test VM..."
  bash scripts/deploy-test-vm.sh {{vm}}

# Rebuild test VM (after deployment)
rebuild-test-vm vm:
  @echo "Rebuilding {{vm}}..."
  nix build .#nixosConfigurations.{{vm}}.config.system.build.toplevel
  @echo "Deploying to VM..."
  # TODO: Add VM deployment command (ssh or qemu)

# SSH into test VM
ssh-test-vm vm port="2222":
  ssh -p {{port}} rain@localhost
```

**Verify**:
```bash
# Test commands
just build-test-vms
just verify-test-vms
just deploy-test-vm sorrow --dry-run
```

**Done when**:
- [ ] Justfile commands added
- [ ] Commands execute without errors
- [ ] Help text works (`just --list`)

## Verification

**After all tasks complete, verify**:

1. **Host Configs Exist**:
   ```bash
   ls hosts/sorrow/default.nix hosts/torment/default.nix
   ```

2. **Builds Succeed**:
   ```bash
   just build-test-vms
   ```

3. **Headless Configuration**:
   ```bash
   # Should return empty array
   nix eval .#nixosConfigurations.sorrow.config.modules.services.desktop --json
   ```

4. **Auto-Upgrade Enabled**:
   ```bash
   # Should return true
   nix eval .#nixosConfigurations.sorrow.config.myModules.services.autoUpgrade.enable
   ```

5. **Golden Generation Enabled**:
   ```bash
   # Should return true
   nix eval .#nixosConfigurations.sorrow.config.myModules.system.boot.goldenGeneration.enable
   ```

6. **Fast Build Time**:
   ```bash
   # Should complete in < 2 minutes
   time nix build .#nixosConfigurations.sorrow.config.system.build.toplevel --no-link --rebuild
   ```

## Files Created/Modified

**Created**:
- `hosts/sorrow/default.nix`
- `hosts/sorrow/hardware-configuration.nix`
- `hosts/torment/default.nix`
- `hosts/torment/hardware-configuration.nix`
- `scripts/deploy-test-vm.sh` (optional)

**Modified**:
- `flake.nix` - Added sorrow and torment to nixosConfigurations
- `justfile` - Added test VM management commands

## Testing Plan (After Deployment)

**Phase 1: Basic Connectivity**
```bash
# Deploy both VMs
just deploy-test-vm sorrow
just deploy-test-vm torment

# SSH into each
ssh -p 2222 rain@localhost  # sorrow
ssh -p 2223 rain@localhost  # torment

# Verify services running
systemctl status sshd tailscaled atuin syncthing
```

**Phase 2: GitOps Testing** (Future - Plan 18-02):
- Multi-host jj concurrent edits
- Auto-upgrade workflow validation
- Network failure testing
- Golden generation rollback

## Notes

**Design Decisions**:
- Used module overrides instead of new role (simpler, less maintenance)
- Hourly auto-upgrade schedule (frequent for testing)
- Minimal services only (SSH, Tailscale, atuin, syncthing)
- No secrets initially (add as needed for testing)

**Future Enhancements** (not in this plan):
- Automated VM deployment via libvirt/virt-manager
- Network isolation for true multi-host testing
- Automated test suite for GitOps workflows
- CI integration for continuous validation
