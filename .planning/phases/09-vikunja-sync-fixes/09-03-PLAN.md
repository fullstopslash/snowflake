---
phase: 09-vikunja-sync-fixes
plan: 03
title: Queue Atomicity & Deduplication
status: pending
---

# Plan 09-03: Queue Atomicity & Deduplication

Fix HIGH priority issue #4: Webhook queue processing lacks atomic operations and proper deduplication.

## Goal

Ensure the retry queue (`queue.txt`) is accessed atomically with proper locking, preventing duplicate entries and lost updates.

## Context

### The Problem

There are TWO queue processors:
1. `processQueueScript` in Nix (shell-based, uses `sort -u`)
2. `cmd_process_queue()` in Python (uses `set(uuids)`)

Both can run concurrently (timer vs manual), and hooks can add to the queue while processing is running:

```
t=0:  process_queue reads queue.txt: [uuid1, uuid2]
t=1:  hook adds uuid1 to queue.txt (retry after failure)
t=2:  process_queue processes uuid1 (succeeds)
t=3:  process_queue removes uuid1 from queue
t=4:  but uuid1 was re-added at t=1 - now lost!
```

### Additional Issues

- Shell script uses `sed -i` which isn't atomic
- Python version snapshots at start, doesn't see new entries
- No locking between shell and Python versions

## Changes

### File: `pkgs/vikunja-sync/vikunja-direct.py`

#### 1. Add queue file locking in `queue_for_retry()`

```python
def queue_for_retry(uuid: str) -> None:
    """Queue a task UUID for retry processing with file locking."""
    queue_file = get_state_dir() / "queue.txt"
    lock_file = get_state_dir() / "queue.lock"

    # Acquire exclusive lock
    with open(lock_file, "w") as lock_fd:
        fcntl.flock(lock_fd, fcntl.LOCK_EX)
        try:
            # Append atomically
            with open(queue_file, "a") as f:
                f.write(f"{uuid}\n")
            log(f"Queued task {uuid} for retry")
        finally:
            fcntl.flock(lock_fd, fcntl.LOCK_UN)
```

#### 2. Rewrite `cmd_process_queue()` with proper locking

```python
def cmd_process_queue(_args: list[str]) -> int:
    """Process the retry queue with proper locking."""
    queue_file = get_state_dir() / "queue.txt"
    lock_file = get_state_dir() / "queue.lock"

    if not queue_file.exists():
        print("Queue is empty")
        return 0

    try:
        config = Config.from_env()
    except ConfigError as e:
        log(f"Config error: {e}")
        return 1

    default_project = os.environ.get("VIKUNJA_DEFAULT_PROJECT", "inbox")
    tw = TaskwarriorClient()

    # Acquire exclusive lock for entire processing
    with open(lock_file, "w") as lock_fd:
        fcntl.flock(lock_fd, fcntl.LOCK_EX)
        try:
            # Read current queue
            if not queue_file.exists():
                return 0

            content = queue_file.read_text().strip()
            if not content:
                queue_file.unlink(missing_ok=True)
                return 0

            uuids = list(dict.fromkeys(line.strip() for line in content.split("\n") if line.strip()))

            processed = 0
            failed = []

            for uuid in uuids:
                task = tw.export_task(uuid)
                if not task:
                    log(f"Task {uuid} not found in TW, removing from queue")
                    continue

                result = push_to_vikunja(task, config, default_project)
                if result["success"]:
                    processed += 1
                    log(f"Synced {uuid}: {result['action']}")
                else:
                    failed.append(uuid)
                    log(f"Failed {uuid}: {result['action']}")

            # Atomic write of remaining items
            if failed:
                queue_file.write_text("\n".join(failed) + "\n")
            else:
                queue_file.unlink(missing_ok=True)

            print(f"Processed: {processed}, Failed: {len(failed)}")
            return 0 if not failed else 1

        finally:
            fcntl.flock(lock_fd, fcntl.LOCK_UN)
```

### File: `roles/vikunja-sync.nix`

#### 3. Update shell-based queue script to use same lock

```nix
processQueueScript = pkgs.writeShellScript "vikunja-process-queue" ''
  STATE_DIR="${stateDir}"
  QUEUE_FILE="$STATE_DIR/queue.txt"
  LOG_FILE="$STATE_DIR/direct.log"
  LOCK_FILE="$STATE_DIR/queue.lock"

  # Ensure state directory exists
  mkdir -p "$STATE_DIR"

  # Exit if no queue file
  [ ! -f "$QUEUE_FILE" ] && exit 0

  # Acquire exclusive lock (skip if another process has it)
  exec 9>"$LOCK_FILE"
  ${pkgs.util-linux}/bin/flock -n 9 || exit 0

  # ... rest of script runs under lock ...
  # Lock released when script exits (fd 9 closes)
'';
```

#### 4. Consolidate to single queue processor (recommended)

Remove the shell-based processor and use Python exclusively:

```nix
# In vikunja-sync-retry service
script = ''
  exec ${vikunjaSync}/bin/vikunja-direct process-queue
'';
```

This eliminates the dual-implementation issue entirely.

## Verification

1. **Concurrent test**: Run `vikunja-direct process-queue` while simultaneously running hooks that add to queue
2. **Stress test**: Add 100 UUIDs to queue from multiple processes, verify none lost
3. **Lock test**: Start queue processing, try to add to queue from another process - should block briefly then succeed

## Checklist

- [ ] Add file locking to `queue_for_retry()`
- [ ] Rewrite `cmd_process_queue()` with lock held for entire operation
- [ ] Update `processQueueScript` in Nix to use same lock file
- [ ] Consider consolidating to Python-only queue processor
- [ ] Verify NixOS rebuild succeeds
- [ ] Manual stress test with concurrent queue access

## Rollback

Revert locking changes. Current behavior is "mostly works" for low-volume usage.
